{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmitize(s):\n",
    "    # Wordnet is an large, freely and publicly available lexical database for the English language \n",
    "    # aiming to establish structured semantic relationships between words. \n",
    "    # It offers lemmatization capabilities as well and is one of the earliest and most commonly used lemmatizers.\n",
    "    #nltk.download('wordnet') #Download this only for the first time\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    # Init the Wordnet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_output = \" \".join([lemmatizer.lemmatize(w) for w in s.split(\" \")])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmitize_spacy(s):\n",
    "    # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Parse the sentence using the loaded 'en' model object `nlp`\n",
    "    doc = nlp(s)\n",
    "\n",
    "    # Extract the lemma for each token and join\n",
    "    spacy_output = \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "    return spacy_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNumbers(s):\n",
    "    s = re.sub(r'\\d+',' ', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveSpecialCharacters(s):\n",
    "    return s.replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveUnicodeChars(s):\n",
    "    #remove the ZERO WIDTH NO-BREAK SPACE unicode character\n",
    "    s = s.replace(u'\\ufeff', '')\n",
    "    s = re.sub(r'[^\\x00-\\x7F]+',' ', s) #Replace non-ASCII characters with a single space\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Document  \\\n",
      "0   trim spotted standstill say stole petersburg ...   \n",
      "1   intermit thrill trim reigneth jocularly spott...   \n",
      "2   devouring thrill trim say apparition tyrant s...   \n",
      "3   thrill dentistry standstill say gazed stole d...   \n",
      "4   mended possess informed trim mockery mainland...   \n",
      "\n",
      "                                 Category  \n",
      "0  AdventuresofHuckleberryFinnbyMarkTwain  \n",
      "1                  MobyDickHermanMelville  \n",
      "2                           taleOf2Cities  \n",
      "3     TheAdventuresofTomSawyerbyMarkTwain  \n",
      "4                               ThePirate  \n"
     ]
    }
   ],
   "source": [
    "def GetBooks():\n",
    "    folderPath = \"txt/\"\n",
    "    files = os.listdir(folderPath)\n",
    "    \n",
    "    corpus = []\n",
    "    labels = []\n",
    "    for file in files:        \n",
    "        with open(folderPath + file, encoding=\"utf8\") as f:\n",
    "            lines = f.readlines()\n",
    "            long_string = \" \".join(lines)\n",
    "            long_string = normalize_document(long_string)\n",
    "            long_string = RemoveUnicodeChars(long_string)\n",
    "            long_string = RemoveNumbers(long_string)\n",
    "            long_string = RemoveSpecialCharacters(long_string)\n",
    "            long_string = Lemmitize(long_string)\n",
    "            long_string = Lemmitize_spacy(long_string)\n",
    "            long_string = \" \".join(list(set(long_string.split(\" \")))) #retain only the unique words\n",
    "            book_name = file.split(\".\")[0] #extract the filename without the extension\n",
    "            labels.append(book_name)\n",
    "            corpus.append(long_string)\n",
    "    corpus = np.array(corpus)\n",
    "    corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
    "    corpus_df = corpus_df[['Document', 'Category']]\n",
    "    return corpus_df\n",
    "\n",
    "corpus_df = GetBooks()\n",
    "print(GetBooks().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ab  aback  abaft  abandon  abandonedly  abandonment  abase  abasement  \\\n",
      "0   1      0      0        0            0            0      0          0   \n",
      "1   1      1      1        1            1            1      1          1   \n",
      "2   1      1      0        1            0            1      0          0   \n",
      "3   0      0      0        1            0            0      0          0   \n",
      "4   1      1      1        1            0            0      0          0   \n",
      "\n",
      "   abash  abashed    ...      zenith  zephyr  zeuglodon  zig  zip  zodiac  \\\n",
      "0      0        0    ...           0       0          0    0    1       0   \n",
      "1      0        1    ...           0       1          1    1    1       1   \n",
      "2      1        0    ...           0       0          0    0    1       0   \n",
      "3      1        0    ...           1       1          0    0    1       0   \n",
      "4      0        0    ...           1       0          0    0    1       0   \n",
      "\n",
      "   zogranda  zone  zoology  zoroaster  \n",
      "0         0     0        0          0  \n",
      "1         1     1        1          1  \n",
      "2         0     0        0          0  \n",
      "3         0     0        0          0  \n",
      "4         0     0        0          0  \n",
      "\n",
      "[5 rows x 18031 columns]\n",
      "18031\n"
     ]
    }
   ],
   "source": [
    "#Bag of Words Model\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(corpus_df[\"Document\"])\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix\n",
    " \n",
    "vocab = cv.get_feature_names()\n",
    "print(pd.DataFrame(cv_matrix, columns=vocab))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the vocabulary into a text file for inspection\n",
    "with open(\"vocabulary.txt\", \"w\") as f:\n",
    "    f.write(re.sub(r',',',\\n', str(vocab)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
