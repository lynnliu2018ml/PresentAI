{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('training_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm only string objects are in the data, by only selecting str types from intial dataset and create a copy of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.loc[df['Tweet'].apply(type) == str].copy()\n",
    "\n",
    "print(\"Old Dataset with all types\", df.count())\n",
    "print(\"New Dataset with only str types\",df_new.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove single characters words (Ex. 'A', 'I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Tweet_Clean'] = df_new['Tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)> 1]))\n",
    "\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does length of tweet affect sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Tweet_Length'] = df_new['Tweet_Clean'].apply(lambda x: len(x))\n",
    "sentence_length = df_new.groupby(['Sentiment']).mean()\n",
    "sentence_length.plot(y='Tweet_Length',kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Feature Column showing the number of \"!\" occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new['Countof!'] = df_new['Tweet_Clean'].str.count(r'([!]+)')\n",
    "\n",
    "#CountofExcalamation = df_new.groupby(['Sentiment']).mean()\n",
    "#sentence_length.plot(y='Tweet_Length',kind='bar')\n",
    "#CountofExcalamation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Feature Column showing the number of at least \"..\"  occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new['Countof.'] = df_new['Tweet_Clean'].str.count(r'([..]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Clouds\n",
    "\n",
    "We want to get an understanding of what are the most common words in all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in df_new['Tweet_Clean']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is a lot of words that you would associate postive sentiment that occure the most (ie. love, good, well, will..)\n",
    "\n",
    "It is harder to see any negative words, we will know look postive labeled tweets and negative labeled tweets seperate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sentiment Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in df_new['Tweet_Clean'][df_new['Sentiment'] == 0]])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the most common occuring words in the negative sentiment are ; Sad, Miss, Sleep, Still, Work, Suck...\n",
    "\n",
    "\n",
    "## Postive Sentiment Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in df_new['Tweet_Clean'][df_new['Sentiment'] == 4]])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the postive word cloud is very similar to the overall word cloud, with most common words being love, go, good, nice ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=100, stop_words='english')\n",
    "cv_matrix = cv.fit_transform(df_new['Tweet_Clean'])\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to See Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "# show document feature vectors\n",
    "BOW = pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_dataset = pd.concat([df_new['Sentiment'], BOW], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of N-Grams Model\n",
    "\n",
    "### 2 Grams Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "bv = CountVectorizer(ngram_range=(2,2),max_df=0.90, max_features=1000, stop_words='english')\n",
    "bv_matrix = bv.fit_transform(df_new['Tweet_Clean'])\n",
    "\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab_2gram = bv.get_feature_names()\n",
    "TwoGram = pd.DataFrame(bv_matrix, columns=vocab_2gram)\n",
    "\n",
    "TwoGram_dataset = pd.concat([df_new['Sentiment'], TwoGram], axis=1, join='inner')\n",
    "TwoGram_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Grams Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "bv = CountVectorizer(ngram_range=(3,3),max_df=0.90, max_features=1000, stop_words='english')\n",
    "bv_matrix = bv.fit_transform(df_new['Tweet_Clean'])\n",
    "\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab_3gram = bv.get_feature_names()\n",
    "ThreeGram = pd.DataFrame(bv_matrix, columns=vocab_3gram)\n",
    "\n",
    "ThreeGram_dataset = pd.concat([df_new['Sentiment'], ThreeGram], axis=1, join='inner')\n",
    "ThreeGram_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TF-IDF Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(max_df=0.90, max_features=1000, stop_words='english', use_idf=True)\n",
    "tv_matrix = tv.fit_transform(df_new['Tweet_Clean'])\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab_tf = tv.get_feature_names()\n",
    "tf_idf = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab_tf)\n",
    "tf_idf_dataset = pd.concat([df_new['Sentiment'], tf_idf], axis=1, join='inner')\n",
    "tf_idf_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some features engineered we want to select which features are most important to identifying the sentiment\n",
    "\n",
    "## Feature Importance\n",
    "\n",
    "We will create multiple random decision trees to help us find our most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "dataframe = tf_idf_dataset\n",
    "array = dataframe.values\n",
    "X = array[:,1:len(array)-1]\n",
    "Y = array[:,0]\n",
    "\n",
    "dataframe.head()\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Select the important features of Model\n",
    "sel = SelectFromModel(model, prefit=True)\n",
    "\n",
    "# Subset features\n",
    "X_new = sel.transform(X)\n",
    "\n",
    "X_new_df = pd.DataFrame(X_new)\n",
    "\n",
    "tf_idf_data_final = pd.concat([df_new['Sentiment'], X_new_df], axis=1, join='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old Number of Features: \" ,len(X[0]))\n",
    "print(\"New Number of Features: \" ,len(X_new[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging TF_IDF and 3GRAM features, dropped sentiment column from ThreeGram, already in tf_idf_dataset\n",
    "dataframe = pd.concat([tf_idf_dataset, ThreeGram_dataset.iloc[:,1:len(ThreeGram_dataset.columns)] ], axis=1, join='inner')\n",
    "array = dataframe.values\n",
    "X = array[:,1:len(array)-1]\n",
    "Y = array[:,0]\n",
    "\n",
    "dataframe.head()\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Select the important features of Model\n",
    "sel = SelectFromModel(model, prefit=True)\n",
    "\n",
    "# Subset features\n",
    "X_mix = sel.transform(X)\n",
    "\n",
    "X_mix_df = pd.DataFrame(X_new)\n",
    "\n",
    "tf_idf_3GRAM_data_final = pd.concat([df_new['Sentiment'], X_mix_df], axis=1, join='inner')\n",
    "\n",
    "print(len(X[0]))\n",
    "print(len(X_mix[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Evaluation\n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Classification Report\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score \n",
    "\n",
    "df_2g_tf = pd.concat([tf_idf_dataset, TwoGram_dataset.iloc[:,1:len(TwoGram_dataset.columns)] ], axis=1, join='inner')\n",
    "\n",
    "metric_names = ['BOW', '2GRAM','3GRAM', 'TF_IDF','TF_IDF_Opt','3GRAM+TF_IDF_OPT','2GRAM+TF_IDF']\n",
    "\n",
    "scores_df = pd.DataFrame(index=metric_names, columns=['roc_auc', 'accuracy']) # to store the scores\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "test_size = 0.2\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "#for metric in metric_names:\n",
    "data = [BOW_dataset,TwoGram_dataset,ThreeGram_dataset,tf_idf_dataset,tf_idf_data_final,tf_idf_3GRAM_data_final,df_2g_tf]\n",
    "count = 0\n",
    "for dataset in data:\n",
    "    metric = metric_names[count]\n",
    "    dataframe = dataset\n",
    "    array = dataframe.values\n",
    "    X = array[:,1:len(array)-1]\n",
    "    Y = array[:,0]\n",
    "    X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "    AUC = cross_val_score(model, X_train, Y_train, scoring='roc_auc', cv=kfold).mean()\n",
    "    accuracy = cross_val_score(model, X_train, Y_train, scoring='accuracy', cv=kfold).mean()\n",
    "    scores_df.loc[metric] = [AUC, accuracy]\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "df_2g_tf = pd.concat([tf_idf_dataset, TwoGram_dataset.iloc[:,1:len(TwoGram_dataset.columns)] ], axis=1, join='inner')\n",
    "\n",
    "metric_names = ['BOW', '2GRAM','3GRAM', 'TF_IDF','TF_IDF_Opt','3GRAM+TF_IDF_OPT','2GRAM+TF_IDF']\n",
    "\n",
    "scores_df = pd.DataFrame(index=metric_names, columns=['roc_auc', 'accuracy']) # to store the scores\n",
    "model = GaussianNB()\n",
    "test_size = 0.2\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "#for metric in metric_names:\n",
    "data = [BOW_dataset,TwoGram_dataset,ThreeGram_dataset,tf_idf_dataset,tf_idf_data_final,tf_idf_3GRAM_data_final,df_2g_tf]\n",
    "count = 0\n",
    "for dataset in data:\n",
    "    metric = metric_names[count]\n",
    "    dataframe = dataset\n",
    "    array = dataframe.values\n",
    "    X = array[:,1:len(array)-1]\n",
    "    Y = array[:,0]\n",
    "    X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "    AUC = cross_val_score(model, X_train, Y_train, scoring='roc_auc', cv=kfold).mean()\n",
    "    accuracy = cross_val_score(model, X_train, Y_train, scoring='accuracy', cv=kfold).mean()\n",
    "    scores_df.loc[metric] = [AUC, accuracy]\n",
    "    print(metric)\n",
    "    print(accuracy)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
