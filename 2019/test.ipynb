@@ -1,1053 +0,0 @@
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relaxin nite drinkin earl grey watchin cool sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cool would nice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate lyn sorry say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>awake bored annoyed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>song day lacey awesome</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentiment\n",
       "0  relaxin nite drinkin earl grey watchin cool sh...          1\n",
       "1                                    cool would nice          1\n",
       "2                                 hate lyn sorry say          0\n",
       "3                                awake bored annoyed          0\n",
       "4                             song day lacey awesome          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plot \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "from sklearn.preprocessing import scale\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('training_cleaned.csv')\n",
    "\n",
    "df[\"Sentiment\"] = df[\"Sentiment\"].map({4:1,0:0})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.loc[df['Tweet'].apply(type) == str].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_new.Tweet\n",
    "y = df_new.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Train,Validate and TEST Sets (80,10,10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 100\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.2, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "n_features = np.arange(10000,50001,10000)\n",
    "vectorizer=cvec\n",
    "checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', lr)\n",
    "        ])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'vectorizer', 'classifier', 'vectorizer__analyzer', 'vectorizer__binary', 'vectorizer__decode_error', 'vectorizer__dtype', 'vectorizer__encoding', 'vectorizer__input', 'vectorizer__lowercase', 'vectorizer__max_df', 'vectorizer__max_features', 'vectorizer__min_df', 'vectorizer__ngram_range', 'vectorizer__preprocessor', 'vectorizer__stop_words', 'vectorizer__strip_accents', 'vectorizer__token_pattern', 'vectorizer__tokenizer', 'vectorizer__vocabulary', 'classifier__C', 'classifier__class_weight', 'classifier__dual', 'classifier__fit_intercept', 'classifier__intercept_scaling', 'classifier__max_iter', 'classifier__multi_class', 'classifier__n_jobs', 'classifier__penalty', 'classifier__random_state', 'classifier__solver', 'classifier__tol', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what Parameters we have to optimize\n",
    "checker_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1) ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1) ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2), total=   0.9s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2), total=   0.9s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2), total=   0.9s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2), total=   0.9s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3), total=   1.4s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3), total=   1.4s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3), total=   1.4s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1), total=   0.5s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1), total=   0.5s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1), total=   0.5s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2), total=   0.9s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3), total=   1.6s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1), total=   0.5s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1), total=   0.5s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1), total=   0.5s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2), total=   1.1s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2), total=   1.1s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3), total=   1.6s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3), total=   1.4s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1), total=   0.5s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1), total=   0.5s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2), total=   1.1s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2), total=   1.1s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2) ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3), total=   1.6s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2), total=   1.0s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2), total=   1.1s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2), total=   1.1s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2), total=   1.1s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3), total=   1.6s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3), total=   1.5s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3), total=   1.6s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3), total=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "       ...ty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)], 'vectorizer__max_features': array([10000, 20000, 30000, 40000, 50000])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize hyperparameters\n",
    "hyperparameters = { \n",
    "                    'vectorizer__ngram_range': [(1,1), (1,2),(1,3)],\n",
    "                   'vectorizer__max_features': n_features,\n",
    "        \n",
    "                    \n",
    "                  }\n",
    "clf = GridSearchCV(checker_pipeline, hyperparameters, cv=5,verbose=2,scoring = 'accuracy')\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizer__max_features': 50000, 'vectorizer__ngram_range': (1, 3)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 77.19%\n"
     ]
    }
   ],
   "source": [
    "clf.refit\n",
    "preds = clf.predict(x_test)\n",
    "BOWaccuracy = accuracy_score(y_test, preds)\n",
    "print(\"accuracy score: {0:.2f}%\".format(BOWaccuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "vectorizer=tfidf\n",
    "checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', lr)\n",
    "        ])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'vectorizer', 'classifier', 'vectorizer__analyzer', 'vectorizer__binary', 'vectorizer__decode_error', 'vectorizer__dtype', 'vectorizer__encoding', 'vectorizer__input', 'vectorizer__lowercase', 'vectorizer__max_df', 'vectorizer__max_features', 'vectorizer__min_df', 'vectorizer__ngram_range', 'vectorizer__norm', 'vectorizer__preprocessor', 'vectorizer__smooth_idf', 'vectorizer__stop_words', 'vectorizer__strip_accents', 'vectorizer__sublinear_tf', 'vectorizer__token_pattern', 'vectorizer__tokenizer', 'vectorizer__use_idf', 'vectorizer__vocabulary', 'classifier__C', 'classifier__class_weight', 'classifier__dual', 'classifier__fit_intercept', 'classifier__intercept_scaling', 'classifier__max_iter', 'classifier__multi_class', 'classifier__n_jobs', 'classifier__penalty', 'classifier__random_state', 'classifier__solver', 'classifier__tol', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what Parameters we have to optimize\n",
    "checker_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1) ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1) ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2), total=   0.7s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2), total=   0.7s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3), total=   1.2s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3), total=   1.2s\n",
      "[CV] vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=10000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2), total=   1.2s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2), total=   0.7s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3), total=   1.4s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=20000, vectorizer__ngram_range=(1, 3), total=   1.2s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2), total=   0.7s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2), total=   0.9s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3), total=   1.2s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=30000, vectorizer__ngram_range=(1, 3), total=   1.4s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 1), total=   0.4s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2), total=   0.9s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2) ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 2), total=   0.9s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3), total=   1.4s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3), total=   1.4s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=40000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1), total=   0.2s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2), total=   0.9s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 2), total=   0.8s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3), total=   1.3s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3), total=   2.0s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3), total=   2.0s\n",
      "[CV] vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3) ..\n",
      "[CV]  vectorizer__max_features=50000, vectorizer__ngram_range=(1, 3), total=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...ty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)], 'vectorizer__max_features': array([10000, 20000, 30000, 40000, 50000])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize hyperparameters\n",
    "hyperparameters = { \n",
    "                    'vectorizer__ngram_range': [(1,1), (1,2),(1,3)],\n",
    "                   'vectorizer__max_features': n_features,\n",
    "        \n",
    "                    \n",
    "                  }\n",
    "clf = GridSearchCV(checker_pipeline, hyperparameters, cv=5,verbose=2,scoring = 'accuracy')\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_IDF_accuracy score: 77.63%\n"
     ]
    }
   ],
   "source": [
    "clf.refit\n",
    "preds = clf.predict(x_test)\n",
    "TF_IDF_accuracy = accuracy_score(y_test, preds)\n",
    "print(\"TF_IDF_accuracy score: {0:.2f}%\".format(TF_IDF_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tblakeley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "#function to create one vector for multiple words by taking average vector of words or sum of vectors\n",
    "\n",
    "def get_w2v_general(tweet, size, vectors, aggregation='mean'):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += vectors[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggregation == 'mean':\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    elif aggregation == 'sum':\n",
    "        return vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre built GLOVE model\n",
    "glove_twitter = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create single tweet vector by averaging each word vector\n",
    "vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_new[\"Tweet\"]]))\n",
    "#create single tweet vector by summing each word vector\n",
    "vecs_glove_sum = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'sum') for z in df_new[\"Tweet\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(vecs_glove_mean, y, test_size=.2, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOVE Linear Regression accuracy score: 75.27%\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "model = lr.fit(x_train,y_train)\n",
    "preds = model.predict(x_test)\n",
    "GLOVE_accuracy = accuracy_score(y_test, preds)\n",
    "print(\"GLOVE Linear Regression accuracy score: {0:.2f}%\".format(GLOVE_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "names = [\"Logistic Regression\", \"Linear SVC\"#,\"Multinomial NB\", \n",
    "          ,\"RandomForest\", \"AdaBoost\",\"KNN\",\"XGBOOST\",\"STACKClassifier\"]\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    #MultinomialNB(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    XGBClassifier(),\n",
    "    StackingClassifier(classifiers=[LogisticRegression(),KNeighborsClassifier()\n",
    "                                    ,XGBClassifier()], \n",
    "                          meta_classifier=lr)\n",
    "    ]\n",
    "zipped_clf = zip(names,classifiers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tblakeley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.27%\n",
      "train and test time: 1.78s\n",
      "--------------------------------------------------------------------------------\n",
      "Model: Linear SVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tblakeley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\tblakeley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.71%\n",
      "train and test time: 19.23s\n",
      "--------------------------------------------------------------------------------\n",
      "Model: RandomForest\n",
      "Accuracy: 65.03%\n",
      "train and test time: 3.70s\n",
      "--------------------------------------------------------------------------------\n",
      "Model: AdaBoost\n",
      "Accuracy: 70.63%\n",
      "train and test time: 38.58s\n",
      "--------------------------------------------------------------------------------\n",
      "Model: KNN\n",
      "Accuracy: 68.30%\n",
      "train and test time: 0.47s\n",
      "--------------------------------------------------------------------------------\n",
      "Model: XGBOOST\n",
      "Accuracy: 72.98%\n",
      "train and test time: 52.58s\n",
      "--------------------------------------------------------------------------------\n",
      "Model: STACKClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tblakeley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import time\n",
    "for n,c in zipped_clf:\n",
    "    print(\"Model: {0}\".format(n))\n",
    "    t0 = time()\n",
    "    c.fit(x_train,y_train)\n",
    "    train_test_time = time() - t0\n",
    "    preds = c.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    print(\"Accuracy: {0:.2f}%\".format(accuracy*100))\n",
    "    print(\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}