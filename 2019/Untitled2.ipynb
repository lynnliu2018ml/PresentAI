{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "import random\n",
    "from sklearn.preprocessing import scale\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('training_cleaned.csv')\n",
    "\n",
    "df[\"Sentiment\"] = df[\"Sentiment\"].map({4:1,0:0})\n",
    "\n",
    "df.head()\n",
    "SEED = random.seed(100)\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.loc[df['Tweet'].apply(type) == str].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_new.Tweet\n",
    "y = df_new.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Train,Validate and TEST Sets (80,10,10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.2, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test,\n",
    "                                                              y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,precision_recall_fscore_support,classification_report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tblakeley\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "#function to create one vector for multiple words by taking average vector of words or sum of vectors\n",
    "\n",
    "def get_w2v_general(tweet, size, vectors, aggregation='mean'):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += vectors[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggregation == 'mean':\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    elif aggregation == 'sum':\n",
    "        return vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre built GLOVE model\n",
    "glove_twitter = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create single tweet vector by averaging each word vector\n",
    "vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_new[\"Tweet\"]]))\n",
    "#create single tweet vector by summing each word vector\n",
    "vecs_glove_sum = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'sum') for z in df_new[\"Tweet\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_x_train, glove_x_validation_and_test, glove_y_train, glove_y_validation_and_test = train_test_split(vecs_glove_mean,\n",
    "                                                                                                          y, test_size=.2, random_state=SEED)\n",
    "glove_x_validation, glove_x_test, glove_y_validation, glove_y_test = train_test_split(glove_x_validation_and_test,\n",
    "                                                                                      glove_y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "names = [\"Logistic Regression\", \"Linear SVC\"#,\"Multinomial NB\", \n",
    "          ,\"RandomForest\", \"AdaBoost\",\"KNN\",\"XGBOOST\",\"STACKClassifier\"]\n",
    "classifiers = [\n",
    "    LogisticRegression(solver='liblinear'),\n",
    "    LinearSVC(),\n",
    "    RandomForestClassifier(n_estimators=200),\n",
    "    AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    XGBClassifier(max_depth=30),\n",
    "    StackingClassifier(classifiers=[LogisticRegression(),RandomForestClassifier(n_estimators=200)\n",
    "                                    ,XGBClassifier(max_depth=30)], \n",
    "                          meta_classifier=LogisticRegression(solver='liblinear'))\n",
    "    ]\n",
    "zipped_clf = zip(names,classifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_All = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for n,c in zipped_clf:\n",
    "    t0 = time()\n",
    "    \n",
    "\n",
    "    # Fit and tune model\n",
    "    #print(clf.estimator)\n",
    "\n",
    "    \n",
    "    clf = c.fit(glove_x_train,glove_y_train)\n",
    "    train_test_time = time() - t0\n",
    "    preds = clf.predict(glove_x_test)\n",
    "    accuracy = accuracy_score(glove_y_test, preds)\n",
    "    model_precision, model_recall, model_fscore, model_support = precision_recall_fscore_support(y_test,preds)\n",
    "    #bestngram = clf.best_params_['vectorizer__ngram_range']\n",
    "    #bestnfeatures = clf.best_params_['vectorizer__max_features']\n",
    "    df_loop = pd.DataFrame([n,\n",
    "                            #bestnfeatures,bestngram,\n",
    "                            accuracy,model_precision[0],model_recall[0],model_fscore[0]])\n",
    "    df_All = df_All.append(df_loop.T, ignore_index = True)\n",
    "    print(\"Model: {0}\".format(n))\n",
    "    #print(\"Features: {0}\".format(clf.best_params_))\n",
    "    print(\"Accuracy: {0:.2f}%\".format(accuracy*100))\n",
    "    print(\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print(\"-\"*80)\n",
    "    df_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)not sure \n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5,verbose = 5)\n",
    "gsearch1.fit(glove_x_train,glove_y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
